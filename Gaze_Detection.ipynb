{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f8d567-afed-4eb2-bd99-053b62814c9d",
   "metadata": {},
   "source": [
    "## Importing Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a84a66-f13b-4065-8466-411539066fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "from hopenet import Hopenet, ResNet  \n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import math\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "num_bins = 66  \n",
    "model = Hopenet(Bottleneck, [3, 4, 6, 3], num_bins) \n",
    "\n",
    "\n",
    "model_path = \"hopenet_robust_alpha1.pkl\"\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "if isinstance(checkpoint, dict):\n",
    "    model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    raise ValueError(\"Invalid checkpoint format: Expected a state_dict but got something else.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461c40b-4519-4f34-9991-2a987cebb536",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5bab7ba-c214-4f05-9b9a-ca9f202c61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # Resize to the input size expected by Hopenet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pretrained models\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811c6e2-de9d-4408-abd5-c4d02f55a5c6",
   "metadata": {},
   "source": [
    "## Pose-estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77a99c57-25be-4a96-8868-f95a93373ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    if len(faces) > 0:\n",
    "        return faces[0]  \n",
    "    return None\n",
    "\n",
    "def estimate_head_pose(frame, face_bbox):\n",
    "    x, y, w, h = face_bbox\n",
    "    face_img = frame[y:y+h, x:x+w] \n",
    "\n",
    "    \n",
    "    face_img = preprocess(face_img).unsqueeze(0)  \n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yaw, pitch, roll = model(face_img)\n",
    "\n",
    "   \n",
    "    idx_tensor = torch.arange(num_bins, dtype=torch.float32).unsqueeze(0)  # Create bin indices\n",
    "    yaw = torch.sum(F.softmax(yaw, dim=1) * idx_tensor, dim=1) * 3 - 99  # Convert to angle in degrees\n",
    "    pitch = torch.sum(F.softmax(pitch, dim=1) * idx_tensor, dim=1) * 3 - 99  # Convert to angle in degrees\n",
    "    roll = torch.sum(F.softmax(roll, dim=1) * idx_tensor, dim=1) * 3 - 99  # Convert to angle in degrees\n",
    "\n",
    "    \n",
    "    yaw = yaw.item()\n",
    "    pitch = pitch.item()\n",
    "    roll = roll.item()\n",
    "\n",
    "    return yaw, pitch, roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b807ff1-f77d-4a5c-84e7-0ddf3f8e9b66",
   "metadata": {},
   "source": [
    "## Condition for Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "834975ab-6b94-4d05-8918-f5ed04595bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_looking_away(yaw, pitch, roll):\n",
    "    \n",
    "    yaw_threshold = 20  # Threshold for left/right head rotation\n",
    "    pitch_threshold = 15  # Threshold for up/down head tilt\n",
    "    roll_threshold = 10  # Threshold for head tilt to the side\n",
    "\n",
    "    # Check if any angle exceeds the threshold\n",
    "    if abs(yaw) > yaw_threshold or abs(pitch) > pitch_threshold or abs(roll) > roll_threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def draw_pose(frame, yaw, pitch, roll, face_bbox):\n",
    "    x, y, w, h = face_bbox\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display head pose angles\n",
    "    cv2.putText(frame, f\"Yaw: {yaw:.2f}\", (x, y-50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Pitch: {pitch:.2f}\", (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Roll: {roll:.2f}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Check if the user is looking away\n",
    "    if is_looking_away(yaw, pitch, roll):\n",
    "        cv2.putText(frame, \"Warning: Looking away!\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)  # Adjusted position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fb4c1-90b4-46bf-81e2-34d73f9e4a44",
   "metadata": {},
   "source": [
    "## Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5051e1d2-1bf6-400c-a07d-f54adcaf08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    face_bbox = detect_face(frame)\n",
    "    if face_bbox is not None:\n",
    "        yaw, pitch, roll = estimate_head_pose(frame, face_bbox)\n",
    "        draw_pose(frame, yaw, pitch, roll, face_bbox)\n",
    "    else:\n",
    "        # Display warning if no face is detected\n",
    "        cv2.putText(frame, \"Warning: No face detected!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Head Pose Estimation\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b5dc6-64b1-467d-ac7a-ceeb239f65bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd38ea3-797d-49ed-a0d1-383556c88460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48691b-5a62-4761-bb08-65bab9f4b4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f610503-0663-4fa2-a580-f6e25153344c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
