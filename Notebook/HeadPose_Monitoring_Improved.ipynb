{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685a5460",
   "metadata": {},
   "source": [
    "# Real-Time Head Pose Monitoring (Webcam) â€” Improved Prototype\n",
    "\n",
    "This notebook contains a **cleaner, more robust** version of your project:\n",
    "- Correct **GPU/CPU device handling**\n",
    "- **MediaPipe** face detection (more robust than Haar cascades)\n",
    "- **Temporal smoothing** (EMA) for stable angles\n",
    "- **Flagging logic** with grace period + sustained-looking-away duration\n",
    "- **Multi-face handling**\n",
    "- **Event logging** to CSV (timestamps, reason, angles)\n",
    "\n",
    "> **Note:** The head-pose model requires weights (e.g., `hopenet_robust_alpha1.pkl`).  \n",
    "Place the file in the notebook directory or update the path in the config cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58086ca3",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "If you're running this locally, install dependencies (uncomment and run).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb719ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need installs in a clean environment.\n",
    "# !pip install opencv-python torch torchvision pillow mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7a5c8",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25192bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Camera\n",
    "    camera_id: int = 0\n",
    "\n",
    "    # Model weights\n",
    "    model_path: str = \"hopenet_robust_alpha1.pkl\"\n",
    "    num_bins: int = 66\n",
    "\n",
    "    # Flag thresholds (degrees)\n",
    "    yaw_thr: float = 20.0\n",
    "    pitch_thr: float = 15.0\n",
    "    roll_thr: float = 10.0\n",
    "\n",
    "    # Temporal logic\n",
    "    ema_alpha: float = 0.2              # smoothing factor (0..1), higher = less smoothing\n",
    "    no_face_grace_s: float = 1.0        # seconds allowed without face before warning\n",
    "    away_required_s: float = 1.2        # looking-away must persist this long to flag\n",
    "\n",
    "    # Logging\n",
    "    log_dir: str = \"logs\"\n",
    "    session_name: str = \"session\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f09f8a",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Robust face detection\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d17453",
   "metadata": {},
   "source": [
    "## 3) Hopenet model loader (with correct device handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# This assumes you have a `hopenet.py` file providing the `Hopenet` module.\n",
    "# If your repo already contains hopenet.py, keep it next to this notebook.\n",
    "# Otherwise, copy your existing Hopenet implementation into hopenet.py.\n",
    "\n",
    "from hopenet import Hopenet  # expects a local file: hopenet.py\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def load_hopenet(model_path: str, num_bins: int, device: torch.device):\n",
    "    model = Hopenet(Bottleneck, [3, 4, 6, 3], num_bins)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if isinstance(checkpoint, dict):\n",
    "        model.load_state_dict(checkpoint)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid checkpoint format. Expected a state_dict dict.\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = load_hopenet(cfg.model_path, cfg.num_bins, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0163a9f",
   "metadata": {},
   "source": [
    "## 4) Preprocessing + MediaPipe face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "mp_face = mp.solutions.face_detection\n",
    "face_detector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "def _clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def detect_faces_mediapipe(frame_bgr):\n",
    "    \"\"\"Return list of (x, y, w, h) in pixel coords.\"\"\"\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detector.process(frame_rgb)\n",
    "    boxes = []\n",
    "    if results.detections:\n",
    "        for det in results.detections:\n",
    "            bb = det.location_data.relative_bounding_box\n",
    "            x = int(bb.xmin * w)\n",
    "            y = int(bb.ymin * h)\n",
    "            bw = int(bb.width * w)\n",
    "            bh = int(bb.height * h)\n",
    "            # Clamp to image bounds\n",
    "            x = _clamp(x, 0, w-1)\n",
    "            y = _clamp(y, 0, h-1)\n",
    "            bw = _clamp(bw, 1, w - x)\n",
    "            bh = _clamp(bh, 1, h - y)\n",
    "            boxes.append((x, y, bw, bh))\n",
    "    return boxes\n",
    "\n",
    "def crop_preprocess_face(frame_bgr, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    face_bgr = frame_bgr[y:y+h, x:x+w]\n",
    "    # Convert to RGB before PIL/torch transforms\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_pil = Image.fromarray(face_rgb)\n",
    "    tensor = preprocess(face_pil)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b1db7",
   "metadata": {},
   "source": [
    "## 5) Head pose estimation + smoothing + flagging + drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_head_pose(model, face_tensor, device: torch.device, num_bins=66):\n",
    "    \"\"\"Return yaw, pitch, roll in degrees (float).\"\"\"\n",
    "    face_tensor = face_tensor.unsqueeze(0).to(device)  # [1,3,224,224]\n",
    "    with torch.no_grad():\n",
    "        yaw_logits, pitch_logits, roll_logits = model(face_tensor)\n",
    "\n",
    "    idx_tensor = torch.arange(num_bins, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    yaw = torch.sum(F.softmax(yaw_logits, dim=1) * idx_tensor, dim=1) * 3 - 99\n",
    "    pitch = torch.sum(F.softmax(pitch_logits, dim=1) * idx_tensor, dim=1) * 3 - 99\n",
    "    roll = torch.sum(F.softmax(roll_logits, dim=1) * idx_tensor, dim=1) * 3 - 99\n",
    "    return float(yaw.item()), float(pitch.item()), float(roll.item())\n",
    "\n",
    "class EMASmoother:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.state = None  # (yaw,pitch,roll)\n",
    "\n",
    "    def update(self, values):\n",
    "        if self.state is None:\n",
    "            self.state = values\n",
    "        else:\n",
    "            a = self.alpha\n",
    "            self.state = tuple((1-a)*s + a*v for s, v in zip(self.state, values))\n",
    "        return self.state\n",
    "\n",
    "def is_looking_away(yaw, pitch, roll, yaw_thr, pitch_thr, roll_thr):\n",
    "    return (abs(yaw) > yaw_thr) or (abs(pitch) > pitch_thr) or (abs(roll) > roll_thr)\n",
    "\n",
    "def draw_overlay(frame, bbox, yaw, pitch, roll, warning_text=None):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    cv2.putText(frame, f\"Yaw: {yaw:6.2f}\", (x, y-50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Pitch:{pitch:6.2f}\", (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Roll: {roll:6.2f}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "\n",
    "    if warning_text:\n",
    "        cv2.putText(frame, warning_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6912424",
   "metadata": {},
   "source": [
    "## 6) CSV event logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_log_path(cfg):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return os.path.join(cfg.log_dir, f\"{cfg.session_name}_{ts}.csv\")\n",
    "\n",
    "log_path = make_log_path(cfg)\n",
    "print(\"Logging to:\", log_path)\n",
    "\n",
    "def log_event(writer, t, event, yaw=None, pitch=None, roll=None, extra=\"\"):\n",
    "    writer.writerow({\n",
    "        \"timestamp_s\": f\"{t:.3f}\",\n",
    "        \"event\": event,\n",
    "        \"yaw\": \"\" if yaw is None else f\"{yaw:.3f}\",\n",
    "        \"pitch\": \"\" if pitch is None else f\"{pitch:.3f}\",\n",
    "        \"roll\": \"\" if roll is None else f\"{roll:.3f}\",\n",
    "        \"extra\": extra\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a44c3",
   "metadata": {},
   "source": [
    "## 7) Run real-time monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time loop\n",
    "cap = cv2.VideoCapture(cfg.camera_id)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open camera_id={cfg.camera_id}\")\n",
    "\n",
    "smoother = EMASmoother(alpha=cfg.ema_alpha)\n",
    "\n",
    "start_t = time.time()\n",
    "last_face_seen_t = start_t\n",
    "\n",
    "away_start_t = None   # when sustained looking-away began\n",
    "is_currently_away = False\n",
    "\n",
    "with open(log_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"timestamp_s\",\"event\",\"yaw\",\"pitch\",\"roll\",\"extra\"])\n",
    "    writer.writeheader()\n",
    "    log_event(writer, 0.0, \"SESSION_START\", extra=f\"device={device}\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - start_t\n",
    "\n",
    "        bboxes = detect_faces_mediapipe(frame)\n",
    "\n",
    "        warning = None\n",
    "\n",
    "        # Multi-face handling\n",
    "        if len(bboxes) > 1:\n",
    "            warning = \"CRITICAL: Multiple faces detected!\"\n",
    "            log_event(writer, t, \"MULTI_FACE\", extra=f\"count={len(bboxes)}\")\n",
    "            # Draw all boxes\n",
    "            for bb in bboxes:\n",
    "                cv2.rectangle(frame, (bb[0], bb[1]), (bb[0]+bb[2], bb[1]+bb[3]), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, warning, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"Head Pose Monitoring\", frame)\n",
    "\n",
    "        elif len(bboxes) == 0:\n",
    "            # No face: allow grace period to avoid false alerts from detection misses\n",
    "            if (now - last_face_seen_t) > cfg.no_face_grace_s:\n",
    "                warning = \"Warning: No face detected!\"\n",
    "                log_event(writer, t, \"NO_FACE\")\n",
    "            cv2.putText(frame, warning or \"No face (within grace period)\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255) if warning else (0, 255, 255), 2)\n",
    "            cv2.imshow(\"Head Pose Monitoring\", frame)\n",
    "\n",
    "            # Reset away tracking if face disappears (policy choice)\n",
    "            away_start_t = None\n",
    "            is_currently_away = False\n",
    "\n",
    "        else:\n",
    "            # Exactly one face\n",
    "            bbox = bboxes[0]\n",
    "            last_face_seen_t = now\n",
    "\n",
    "            face_tensor = crop_preprocess_face(frame, bbox)\n",
    "            yaw, pitch, roll = estimate_head_pose(model, face_tensor, device, num_bins=cfg.num_bins)\n",
    "\n",
    "            # Smooth angles\n",
    "            yaw_s, pitch_s, roll_s = smoother.update((yaw, pitch, roll))\n",
    "\n",
    "            # Away logic\n",
    "            away_now = is_looking_away(yaw_s, pitch_s, roll_s, cfg.yaw_thr, cfg.pitch_thr, cfg.roll_thr)\n",
    "\n",
    "            if away_now and not is_currently_away:\n",
    "                # Just started deviating; start timer\n",
    "                away_start_t = now\n",
    "                is_currently_away = True\n",
    "                log_event(writer, t, \"AWAY_START\", yaw_s, pitch_s, roll_s)\n",
    "\n",
    "            if away_now and is_currently_away:\n",
    "                # Has it lasted long enough to flag?\n",
    "                if away_start_t and (now - away_start_t) >= cfg.away_required_s:\n",
    "                    warning = \"Warning: Looking away!\"\n",
    "                    log_event(writer, t, \"AWAY_FLAG\", yaw_s, pitch_s, roll_s, extra=f\"duration_s={now-away_start_t:.2f}\")\n",
    "\n",
    "            if (not away_now) and is_currently_away:\n",
    "                # Returned to normal\n",
    "                log_event(writer, t, \"AWAY_END\", yaw_s, pitch_s, roll_s, extra=f\"duration_s={now-away_start_t:.2f}\" if away_start_t else \"\")\n",
    "                away_start_t = None\n",
    "                is_currently_away = False\n",
    "\n",
    "            draw_overlay(frame, bbox, yaw_s, pitch_s, roll_s, warning_text=warning)\n",
    "            cv2.imshow(\"Head Pose Monitoring\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            log_event(writer, time.time()-start_t, \"SESSION_END\")\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Done. Log saved to:\", log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffcfc2",
   "metadata": {},
   "source": [
    "## 8) Quick log preview\n",
    "\n",
    "Run the next cell to see the first few log entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(log_path)\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
