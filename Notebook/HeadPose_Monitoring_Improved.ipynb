{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685a5460",
   "metadata": {},
   "source": [
    "# Real-Time Head Pose Monitoring (Webcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58086ca3",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb719ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need installs in a clean environment.\n",
    "# !pip install opencv-python torch torchvision pillow mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7a5c8",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25192bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(camera_id=0, model_path='hopenet_robust_alpha1.pkl', num_bins=66, yaw_thr=20.0, pitch_thr=15.0, roll_thr=10.0, ema_alpha=0.2, no_face_grace_s=1.0, away_required_s=1.2, log_dir='logs', session_name='session')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Camera\n",
    "    camera_id: int = 0\n",
    "\n",
    "    # Model weights\n",
    "    model_path: str = \"hopenet_robust_alpha1.pkl\"\n",
    "    num_bins: int = 66\n",
    "\n",
    "    # Flag thresholds (degrees)\n",
    "    yaw_thr: float = 20.0\n",
    "    pitch_thr: float = 15.0\n",
    "    roll_thr: float = 10.0\n",
    "\n",
    "    # Temporal logic\n",
    "    ema_alpha: float = 0.2              # smoothing factor (0..1), higher = less smoothing\n",
    "    no_face_grace_s: float = 1.0        # seconds allowed without face before warning\n",
    "    away_required_s: float = 1.2        # looking-away must persist this long to flag\n",
    "\n",
    "    # Logging\n",
    "    log_dir: str = \"logs\"\n",
    "    session_name: str = \"session\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f09f8a",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3e7bb",
   "metadata": {},
   "source": [
    "## Hopenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659ad455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Hopenet(nn.Module):\n",
    "    # Hopenet with 3 output layers for yaw, pitch and roll\n",
    "    # Predicts Euler angles by binning and regression with the expected value\n",
    "    def __init__(self, block, layers, num_bins):\n",
    "        self.inplanes = 64\n",
    "        super(Hopenet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fc_yaw = nn.Linear(512 * block.expansion, num_bins)\n",
    "        self.fc_pitch = nn.Linear(512 * block.expansion, num_bins)\n",
    "        self.fc_roll = nn.Linear(512 * block.expansion, num_bins)\n",
    "\n",
    "        # Vestigial layer from previous experiments\n",
    "        self.fc_finetune = nn.Linear(512 * block.expansion + 3, 3)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        pre_yaw = self.fc_yaw(x)\n",
    "        pre_pitch = self.fc_pitch(x)\n",
    "        pre_roll = self.fc_roll(x)\n",
    "\n",
    "        return pre_yaw, pre_pitch, pre_roll\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    # ResNet for regression of 3 Euler angles.\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fc_angles = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_angles(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    # AlexNet laid out as a Hopenet - classify Euler angles in bins and\n",
    "    # regress the expected value.\n",
    "    def __init__(self, num_bins):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc_yaw = nn.Linear(4096, num_bins)\n",
    "        self.fc_pitch = nn.Linear(4096, num_bins)\n",
    "        self.fc_roll = nn.Linear(4096, num_bins)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        yaw = self.fc_yaw(x)\n",
    "        pitch = self.fc_pitch(x)\n",
    "        roll = self.fc_roll(x)\n",
    "        return yaw, pitch, roll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d17453",
   "metadata": {},
   "source": [
    "## 3) Hopenet model loader (with correct device handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d8c4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def load_hopenet(model_path: str, num_bins: int, device: torch.device):\n",
    "    model = Hopenet(Bottleneck, [3, 4, 6, 3], num_bins)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if isinstance(checkpoint, dict):\n",
    "        model.load_state_dict(checkpoint)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid checkpoint format. Expected a state_dict dict.\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = load_hopenet(cfg.model_path, cfg.num_bins, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0163a9f",
   "metadata": {},
   "source": [
    "## 4) Preprocessing + MediaPipe face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d5bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1772212933.244880   55500 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1772212933.247758   55689 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.2.8-0ubuntu0.24.04.1), renderer: AMD Radeon 780M Graphics (radeonsi, phoenix, LLVM 20.1.2, DRM 3.64, 6.17.0-14-generic)\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "mp_face = mp.solutions.face_detection\n",
    "face_detector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "def _clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def detect_faces_mediapipe(frame_bgr):\n",
    "    \"\"\"Return list of (x, y, w, h) in pixel coords.\"\"\"\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detector.process(frame_rgb)\n",
    "    boxes = []\n",
    "    if results.detections:\n",
    "        for det in results.detections:\n",
    "            bb = det.location_data.relative_bounding_box\n",
    "            x = int(bb.xmin * w)\n",
    "            y = int(bb.ymin * h)\n",
    "            bw = int(bb.width * w)\n",
    "            bh = int(bb.height * h)\n",
    "            # Clamp to image bounds\n",
    "            x = _clamp(x, 0, w-1)\n",
    "            y = _clamp(y, 0, h-1)\n",
    "            bw = _clamp(bw, 1, w - x)\n",
    "            bh = _clamp(bh, 1, h - y)\n",
    "            boxes.append((x, y, bw, bh))\n",
    "    return boxes\n",
    "\n",
    "def crop_preprocess_face(frame_bgr, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    face_bgr = frame_bgr[y:y+h, x:x+w]\n",
    "    # Convert to RGB before PIL/torch transforms\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_pil = Image.fromarray(face_rgb)\n",
    "    tensor = preprocess(face_pil)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b1db7",
   "metadata": {},
   "source": [
    "## 5) Head pose estimation + smoothing + flagging + drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6312381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1772212933.263426   55675 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def estimate_head_pose(model, face_tensor, device: torch.device, num_bins=66):\n",
    "    \"\"\"Return yaw, pitch, roll in degrees (float).\"\"\"\n",
    "    face_tensor = face_tensor.unsqueeze(0).to(device)  # [1,3,224,224]\n",
    "    with torch.no_grad():\n",
    "        yaw_logits, pitch_logits, roll_logits = model(face_tensor)\n",
    "\n",
    "    idx_tensor = torch.arange(num_bins, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    yaw = torch.sum(F.softmax(yaw_logits, dim=1) * idx_tensor, dim=1) * 3 - 99\n",
    "    pitch = torch.sum(F.softmax(pitch_logits, dim=1) * idx_tensor, dim=1) * 3 - 99\n",
    "    roll = torch.sum(F.softmax(roll_logits, dim=1) * idx_tensor, dim=1) * 3 - 99\n",
    "    return float(yaw.item()), float(pitch.item()), float(roll.item())\n",
    "\n",
    "class EMASmoother:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.state = None  # (yaw,pitch,roll)\n",
    "\n",
    "    def update(self, values):\n",
    "        if self.state is None:\n",
    "            self.state = values\n",
    "        else:\n",
    "            a = self.alpha\n",
    "            self.state = tuple((1-a)*s + a*v for s, v in zip(self.state, values))\n",
    "        return self.state\n",
    "\n",
    "def is_looking_away(yaw, pitch, roll, yaw_thr, pitch_thr, roll_thr):\n",
    "    return (abs(yaw) > yaw_thr) or (abs(pitch) > pitch_thr) or (abs(roll) > roll_thr)\n",
    "\n",
    "def draw_overlay(frame, bbox, yaw, pitch, roll, warning_text=None):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    cv2.putText(frame, f\"Yaw: {yaw:6.2f}\", (x, y-50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Pitch:{pitch:6.2f}\", (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Roll: {roll:6.2f}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "\n",
    "    if warning_text:\n",
    "        cv2.putText(frame, warning_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6912424",
   "metadata": {},
   "source": [
    "## 6) CSV event logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3271075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: logs/session_20260227_182213.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def make_log_path(cfg):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return os.path.join(cfg.log_dir, f\"{cfg.session_name}_{ts}.csv\")\n",
    "\n",
    "log_path = make_log_path(cfg)\n",
    "print(\"Logging to:\", log_path)\n",
    "\n",
    "def log_event(writer, t, event, yaw=None, pitch=None, roll=None, extra=\"\"):\n",
    "    writer.writerow({\n",
    "        \"timestamp_s\": f\"{t:.3f}\",\n",
    "        \"event\": event,\n",
    "        \"yaw\": \"\" if yaw is None else f\"{yaw:.3f}\",\n",
    "        \"pitch\": \"\" if pitch is None else f\"{pitch:.3f}\",\n",
    "        \"roll\": \"\" if roll is None else f\"{roll:.3f}\",\n",
    "        \"extra\": extra\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a44c3",
   "metadata": {},
   "source": [
    "## 7) Run real-time monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abc63b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokul-o-s/Desktop/Real-Time-VideoMonitor/real_t/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "QFontDatabase: Cannot find font directory /home/gokul-o-s/Desktop/Real-Time-VideoMonitor/real_t/lib/python3.11/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/gokul-o-s/Desktop/Real-Time-VideoMonitor/real_t/lib/python3.11/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/gokul-o-s/Desktop/Real-Time-VideoMonitor/real_t/lib/python3.11/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/gokul-o-s/Desktop/Real-Time-VideoMonitor/real_t/lib/python3.11/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/gokul-o-s/Desktop/Real-Time-VideoMonitor/real_t/lib/python3.11/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Log saved to: logs/session_20260227_182213.csv\n"
     ]
    }
   ],
   "source": [
    "# Real-time loop\n",
    "cap = cv2.VideoCapture(cfg.camera_id)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open camera_id={cfg.camera_id}\")\n",
    "\n",
    "smoother = EMASmoother(alpha=cfg.ema_alpha)\n",
    "\n",
    "start_t = time.time()\n",
    "last_face_seen_t = start_t\n",
    "\n",
    "away_start_t = None   # when sustained looking-away began\n",
    "is_currently_away = False\n",
    "\n",
    "with open(log_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"timestamp_s\",\"event\",\"yaw\",\"pitch\",\"roll\",\"extra\"])\n",
    "    writer.writeheader()\n",
    "    log_event(writer, 0.0, \"SESSION_START\", extra=f\"device={device}\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - start_t\n",
    "\n",
    "        bboxes = detect_faces_mediapipe(frame)\n",
    "\n",
    "        warning = None\n",
    "\n",
    "        # Multi-face handling\n",
    "        if len(bboxes) > 1:\n",
    "            warning = \"CRITICAL: Multiple faces detected!\"\n",
    "            log_event(writer, t, \"MULTI_FACE\", extra=f\"count={len(bboxes)}\")\n",
    "            # Draw all boxes\n",
    "            for bb in bboxes:\n",
    "                cv2.rectangle(frame, (bb[0], bb[1]), (bb[0]+bb[2], bb[1]+bb[3]), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, warning, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"Head Pose Monitoring\", frame)\n",
    "\n",
    "        elif len(bboxes) == 0:\n",
    "            # No face: allow grace period to avoid false alerts from detection misses\n",
    "            if (now - last_face_seen_t) > cfg.no_face_grace_s:\n",
    "                warning = \"Warning: No face detected!\"\n",
    "                log_event(writer, t, \"NO_FACE\")\n",
    "            cv2.putText(frame, warning or \"No face (within grace period)\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255) if warning else (0, 255, 255), 2)\n",
    "            cv2.imshow(\"Head Pose Monitoring\", frame)\n",
    "\n",
    "            # Reset away tracking if face disappears\n",
    "            away_start_t = None\n",
    "            is_currently_away = False\n",
    "\n",
    "        else:\n",
    "            # Exactly one face\n",
    "            bbox = bboxes[0]\n",
    "            last_face_seen_t = now\n",
    "\n",
    "            face_tensor = crop_preprocess_face(frame, bbox)\n",
    "            yaw, pitch, roll = estimate_head_pose(model, face_tensor, device, num_bins=cfg.num_bins)\n",
    "\n",
    "            # Smooth angles\n",
    "            yaw_s, pitch_s, roll_s = smoother.update((yaw, pitch, roll))\n",
    "\n",
    "            # Away logic\n",
    "            away_now = is_looking_away(yaw_s, pitch_s, roll_s, cfg.yaw_thr, cfg.pitch_thr, cfg.roll_thr)\n",
    "\n",
    "            if away_now and not is_currently_away:\n",
    "                # Just started deviating; start timer\n",
    "                away_start_t = now\n",
    "                is_currently_away = True\n",
    "                log_event(writer, t, \"AWAY_START\", yaw_s, pitch_s, roll_s)\n",
    "\n",
    "            if away_now and is_currently_away:\n",
    "                # Has it lasted long enough to flag?\n",
    "                if away_start_t and (now - away_start_t) >= cfg.away_required_s:\n",
    "                    warning = \"Warning: Looking away!\"\n",
    "                    log_event(writer, t, \"AWAY_FLAG\", yaw_s, pitch_s, roll_s, extra=f\"duration_s={now-away_start_t:.2f}\")\n",
    "\n",
    "            if (not away_now) and is_currently_away:\n",
    "                # Returned to normal\n",
    "                log_event(writer, t, \"AWAY_END\", yaw_s, pitch_s, roll_s, extra=f\"duration_s={now-away_start_t:.2f}\" if away_start_t else \"\")\n",
    "                away_start_t = None\n",
    "                is_currently_away = False\n",
    "\n",
    "            draw_overlay(frame, bbox, yaw_s, pitch_s, roll_s, warning_text=warning)\n",
    "            cv2.imshow(\"Head Pose Monitoring\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            log_event(writer, time.time()-start_t, \"SESSION_END\")\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Done. Log saved to:\", log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffcfc2",
   "metadata": {},
   "source": [
    "## 8) Quick log preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f6d091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-3.0.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.0 in ./real_t/lib/python3.11/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./real_t/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./real_t/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-3.0.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "Successfully installed pandas-3.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_s</th>\n",
       "      <th>event</th>\n",
       "      <th>yaw</th>\n",
       "      <th>pitch</th>\n",
       "      <th>roll</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>SESSION_START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>device=cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.133</td>\n",
       "      <td>AWAY_START</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>-15.572</td>\n",
       "      <td>-2.389</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.533</td>\n",
       "      <td>AWAY_END</td>\n",
       "      <td>3.277</td>\n",
       "      <td>-13.986</td>\n",
       "      <td>-7.002</td>\n",
       "      <td>duration_s=0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.237</td>\n",
       "      <td>AWAY_START</td>\n",
       "      <td>20.549</td>\n",
       "      <td>0.683</td>\n",
       "      <td>-5.442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.901</td>\n",
       "      <td>AWAY_END</td>\n",
       "      <td>15.523</td>\n",
       "      <td>3.233</td>\n",
       "      <td>-7.863</td>\n",
       "      <td>duration_s=0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.137</td>\n",
       "      <td>AWAY_START</td>\n",
       "      <td>4.991</td>\n",
       "      <td>16.020</td>\n",
       "      <td>-3.011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.105</td>\n",
       "      <td>AWAY_END</td>\n",
       "      <td>5.036</td>\n",
       "      <td>14.893</td>\n",
       "      <td>-3.538</td>\n",
       "      <td>duration_s=0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.737</td>\n",
       "      <td>AWAY_START</td>\n",
       "      <td>-21.690</td>\n",
       "      <td>11.970</td>\n",
       "      <td>-2.419</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.969</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-71.412</td>\n",
       "      <td>-23.709</td>\n",
       "      <td>27.258</td>\n",
       "      <td>duration_s=1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.005</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-71.283</td>\n",
       "      <td>-23.586</td>\n",
       "      <td>27.276</td>\n",
       "      <td>duration_s=1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18.037</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-70.876</td>\n",
       "      <td>-23.579</td>\n",
       "      <td>27.451</td>\n",
       "      <td>duration_s=1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.069</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-69.780</td>\n",
       "      <td>-23.096</td>\n",
       "      <td>27.000</td>\n",
       "      <td>duration_s=1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18.105</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-68.870</td>\n",
       "      <td>-22.394</td>\n",
       "      <td>26.343</td>\n",
       "      <td>duration_s=1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.137</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-66.816</td>\n",
       "      <td>-22.671</td>\n",
       "      <td>26.735</td>\n",
       "      <td>duration_s=1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.169</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-64.404</td>\n",
       "      <td>-21.590</td>\n",
       "      <td>26.656</td>\n",
       "      <td>duration_s=1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18.205</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-61.529</td>\n",
       "      <td>-19.855</td>\n",
       "      <td>25.333</td>\n",
       "      <td>duration_s=1.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.237</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-57.776</td>\n",
       "      <td>-16.957</td>\n",
       "      <td>23.236</td>\n",
       "      <td>duration_s=1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.269</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-54.166</td>\n",
       "      <td>-13.927</td>\n",
       "      <td>21.070</td>\n",
       "      <td>duration_s=1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.305</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-50.968</td>\n",
       "      <td>-10.991</td>\n",
       "      <td>19.198</td>\n",
       "      <td>duration_s=1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.337</td>\n",
       "      <td>AWAY_FLAG</td>\n",
       "      <td>-47.564</td>\n",
       "      <td>-8.591</td>\n",
       "      <td>17.232</td>\n",
       "      <td>duration_s=1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp_s          event     yaw   pitch    roll            extra\n",
       "0         0.000  SESSION_START     NaN     NaN     NaN      device=cuda\n",
       "1         9.133     AWAY_START  -1.129 -15.572  -2.389              NaN\n",
       "2         9.533       AWAY_END   3.277 -13.986  -7.002  duration_s=0.40\n",
       "3        11.237     AWAY_START  20.549   0.683  -5.442              NaN\n",
       "4        11.901       AWAY_END  15.523   3.233  -7.863  duration_s=0.66\n",
       "5        15.137     AWAY_START   4.991  16.020  -3.011              NaN\n",
       "6        16.105       AWAY_END   5.036  14.893  -3.538  duration_s=0.97\n",
       "7        16.737     AWAY_START -21.690  11.970  -2.419              NaN\n",
       "8        17.969      AWAY_FLAG -71.412 -23.709  27.258  duration_s=1.23\n",
       "9        18.005      AWAY_FLAG -71.283 -23.586  27.276  duration_s=1.27\n",
       "10       18.037      AWAY_FLAG -70.876 -23.579  27.451  duration_s=1.30\n",
       "11       18.069      AWAY_FLAG -69.780 -23.096  27.000  duration_s=1.33\n",
       "12       18.105      AWAY_FLAG -68.870 -22.394  26.343  duration_s=1.37\n",
       "13       18.137      AWAY_FLAG -66.816 -22.671  26.735  duration_s=1.40\n",
       "14       18.169      AWAY_FLAG -64.404 -21.590  26.656  duration_s=1.43\n",
       "15       18.205      AWAY_FLAG -61.529 -19.855  25.333  duration_s=1.47\n",
       "16       18.237      AWAY_FLAG -57.776 -16.957  23.236  duration_s=1.50\n",
       "17       18.269      AWAY_FLAG -54.166 -13.927  21.070  duration_s=1.53\n",
       "18       18.305      AWAY_FLAG -50.968 -10.991  19.198  duration_s=1.57\n",
       "19       18.337      AWAY_FLAG -47.564  -8.591  17.232  duration_s=1.60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(log_path)\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_t (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
